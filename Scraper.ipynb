{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6ff863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8290dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e76b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://evolution-outreach.biomedcentral.com/articles?searchType=journalSearch&sort=PubDate&page=1'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf6b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept the cookies\n",
    "browser.find_by_css('button').first.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f1dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4938a52e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28008\\3750089211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoupe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ol\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c-listing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpapers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"li\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c-listing__item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "html = browser.html\n",
    "\n",
    "soupe = soup(html, 'html.parser')\n",
    "\n",
    "articles = soupe.find(\"ol\", class_='c-listing')\n",
    "\n",
    "papers = articles.find_all(\"li\", class_='c-listing__item')\n",
    "\n",
    "for paper in papers:\n",
    "    title = paper.a.text\n",
    "    link = paper.a['href']\n",
    "    authors_string = paper.find(\"span\",class_=\"c-listing__authors-list\").text\n",
    "    authors = authors_string.replace(\" and\", \",\").split(\", \")\n",
    "    metadata=[]\n",
    "    metadata_ = paper.find(\"div\",class_=\"c-listing__metadata\")\n",
    "    for data in metadata_:\n",
    "        metadata.append(data)\n",
    "    content_type = metadata[1].text.lstrip('Content type').strip(\": \")\n",
    "    date = metadata[3].text.lstrip('Published on: ')\n",
    "    \n",
    "    body = get_article_text(link)\n",
    "    \n",
    "    # Create dictionary of the artilce's information\n",
    "\n",
    "    paper_info = {}\n",
    "    paper_info['Title']= title\n",
    "    paper_info['Authors']=authors\n",
    "    paper_info['Content Type'] = content_type\n",
    "    paper_info['Published date'] = date\n",
    "    paper_info['Article text'] = body\n",
    "\n",
    "    # Append the dictionary to the article list\n",
    "\n",
    "    articles_list.append(paper_info)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a609c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.links.find_by_href(link).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c59b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_article_text(link):\n",
    "    browser.links.find_by_href(link).click()\n",
    "\n",
    "    sleep(1)\n",
    "\n",
    "    paper_body = []\n",
    "\n",
    "    paper_html = browser.html\n",
    "\n",
    "    article_text = []\n",
    "\n",
    "    soupy = soup(paper_html, 'html.parser')\n",
    "\n",
    "\n",
    "    browser.back()\n",
    "\n",
    "    return(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2615fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleted this from the get_article_text\n",
    "\n",
    "\n",
    "    article_ = soupy.find_all('article')[0]\n",
    "\n",
    "    start = article_.find_all('div', class_='c-article-section__content c-article-section__content--separator')[0].p.text\n",
    "    body_paragraphes = article_.find_all('div', class_='c-article-section__content')\n",
    "    article_text.append(start)\n",
    "    for body in body_paragraphes:\n",
    "        article_text.append(body.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soupy = soup(paper_html, 'html.parser')\n",
    "print(soupy)\n",
    "soupy.find_all(\"div\", class_='c=article-section__content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbfe94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for paragraphe in written_paper:\n",
    "    print(paragraphe.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6365773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary():\n",
    "\n",
    "    html = browser.html\n",
    "\n",
    "    soupe = soup(html, 'html.parser')\n",
    "\n",
    "    articles = soupe.find(\"ol\", class_='c-listing')\n",
    "\n",
    "    papers = articles.find_all(\"li\", class_='c-listing__item')\n",
    "\n",
    "    for paper in papers:\n",
    "        title = paper.a.text\n",
    "        link = paper.a['href']\n",
    "        authors_string = paper.find(\"span\",class_=\"c-listing__authors-list\").text\n",
    "        authors = authors_string.replace(\" and\", \",\").split(\", \")\n",
    "        metadata=[]\n",
    "        metadata_ = paper.find(\"div\",class_=\"c-listing__metadata\")\n",
    "        for data in metadata_:\n",
    "            metadata.append(data)\n",
    "        content_type = metadata[1].text.lstrip('Content type').strip(\": \")\n",
    "        date = metadata[3].text.lstrip('Published on: ')\n",
    "\n",
    "        body = get_article_text(link)\n",
    "\n",
    "        # Create dictionary of the artilce's information\n",
    "\n",
    "        paper_info = {}\n",
    "        paper_info['Title']= title\n",
    "        paper_info['Authors']=authors\n",
    "        paper_info['Content Type'] = content_type\n",
    "        paper_info['Published date'] = date\n",
    "        paper_info['Article text'] = body\n",
    "\n",
    "        # Append the dictionary to the article list\n",
    "\n",
    "        articles_list.append(paper_info)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ceca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary():\n",
    "\n",
    "    html = browser.html\n",
    "\n",
    "    soupe = soup(html, 'html.parser')\n",
    "\n",
    "    articles = soupe.find(\"ol\", class_='c-listing')\n",
    "\n",
    "    papers = articles.find_all(\"li\", class_='c-listing__item')\n",
    "\n",
    "    for paper in papers:\n",
    "        title = paper.a.text\n",
    "        link = paper.a['href']\n",
    "        authors_string = paper.find(\"span\",class_=\"c-listing__authors-list\").text\n",
    "        authors = authors_string.replace(\" and\", \",\").split(\", \")\n",
    "        metadata=[]\n",
    "        metadata_ = paper.find(\"div\",class_=\"c-listing__metadata\")\n",
    "        for data in metadata_:\n",
    "            metadata.append(data)\n",
    "        content_type = metadata[1].text.lstrip('Content type').strip(\": \")\n",
    "        date = metadata[3].text.lstrip('Published on: ')\n",
    "        \n",
    "        # Create dictionary of the artilce's information\n",
    "        \n",
    "        paper_info = {}\n",
    "        paper_info['Title']= title\n",
    "        paper_info['Authors']=authors\n",
    "        paper_info['Content Type'] = content_type\n",
    "        paper_info['Published date'] = date\n",
    "        \n",
    "        # Append the dictionary to the article list\n",
    "        \n",
    "        articles_list.append(paper_info)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6023398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_body(link):\n",
    "    \n",
    "    browser.links.find_by_href(link).click()\n",
    "    \n",
    "    paper_body = []\n",
    "    \n",
    "    paper_html = browser.html\n",
    "    \n",
    "    soupy = soup(html, 'html.parser')\n",
    "    \n",
    "    written_paper = soupy.find_all(\"article\")\n",
    "    \n",
    "    for paragraphe in written_paper:\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69387293",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28008\\1638093626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mget_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_partial_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Next\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28008\\2705188334.py\u001b[0m in \u001b[0;36mget_summary\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoupe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ol\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c-listing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpapers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"li\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c-listing__item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for _ in range(13):\n",
    "    get_summary()\n",
    "    browser.links.find_by_partial_text(\"Next\").click()\n",
    "get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016102a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde216f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
